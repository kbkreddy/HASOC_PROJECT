{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from nltk.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data and Test data with Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english_amz'))\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "LINKS =re.compile('(?i)(http(s)?[:/.a-z0-9]*)')\n",
    "CHAR_OCC = re.compile(r'(.)\\1{2,}')\n",
    "#LINKS = re.compile(\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = LINKS.sub('', text)\n",
    "    text = CHAR_OCC.sub(r'\\1',text) # replace more than 2 occurences of a same char like aaaaabbbbbbb to ab\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    text = re.sub(\"@[a-zA-Z]*\", \"\", text) # remove usernames with @******\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = re.sub(\"\\'\", \"\", text) # remove backslash-apostrophe \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) # remove everything except alphabets\n",
    "    text = re.sub(\"( |^)[a-zA-Z][a-zA-Z]? \",\" \",text) # remove words with just 1 or 2 letters\n",
    "    text = ' '.join(text.split()) # remove whitespaces \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training svm model for TASK1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding label column\n",
    "df_train['label_t1'] = df_train['task1'].apply(lambda x: 0 if x=='NOT' else 1)\n",
    "df_test['label_t1'] = df_test['task1'].apply(lambda x: 0 if x=='NOT' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['label_t1'].values\n",
    "\n",
    "cv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True, stop_words='english')\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "SVM_Task1 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\n",
    "SVM_Task1.fit(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.93      0.91       370\n",
      "          1       0.92      0.87      0.90       338\n",
      "\n",
      "avg / total       0.90      0.90      0.90       708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_SVM_Task1 = SVM_Task1.predict(cv.transform(df_test['text'].values))\n",
    "df_test['svm-task1'] = predictions_SVM_Task1.tolist()\n",
    "print(classification_report(df_test['label_t1'],predictions_SVM_Task1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        HOF       0.93      0.86      0.89       338\n",
      "        NOT       0.88      0.94      0.91       370\n",
      "\n",
      "avg / total       0.90      0.90      0.90       708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold based\n",
    "svm_pred = SVM_Task1.predict_proba(cv.transform(df_test['text'].values))\n",
    "threshold = 0.6\n",
    "thresh = lambda x: 'HOF' if x[1]>threshold else 'NOT'\n",
    "y_pred = [thresh(x) for x in svm_pred]\n",
    "print(classification_report(df_test['task1'],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.95      0.92       370\n",
      "          1       0.94      0.87      0.90       338\n",
      "\n",
      "avg / total       0.91      0.91      0.91       708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['label_t1'].values\n",
    "\n",
    "cv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True, stop_words='english')\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "LR_Task1 = LogisticRegression()\n",
    "LR_Task1.fit(X_train_cv, y_train)\n",
    "\n",
    "predictions_LR_Task1 = LR_Task1.predict(cv.transform(df_test['text'].values))\n",
    "df_test['lr-task1'] = predictions_LR_Task1.tolist()\n",
    "print(classification_report(df_test['label_t1'],predictions_LR_Task1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading rules with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "#ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_words = set()\n",
    "offense_words_stemmed = set()\n",
    "with open('bad-words.txt') as f:\n",
    "    for line in f:\n",
    "        offense_words.add(clean_text(line.replace(\"\\n\",'')))\n",
    "        offense_words_stemmed.add(stemmer.stem(clean_text(line.replace(\"\\n\",''))))\n",
    "offense_words_stemmed = [x for x in offense_words_stemmed if x != '']\n",
    "offense_words = [x for x in offense_words if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contains_offensive_words(text,count,stem):\n",
    "#     text_set = set(word_tokenize(text))\n",
    "#     text_set_stemmed = set([stemmer.stem(x) for x in word_tokenize(text)])\n",
    "#     if stem:\n",
    "#         matched_words = text_set_stemmed.intersection(offense_words_stemmed)\n",
    "#     else:\n",
    "#         matched_words = text_set.intersection(offense_words)\n",
    "#     if len(matched_words) >= count:\n",
    "#         return 'HOF'\n",
    "#     else:\n",
    "#         return 'NOT'\n",
    "\n",
    "def distance_match(text_set,offence_set,distance):\n",
    "    count = 0\n",
    "    for x in text_set:\n",
    "        if x != \"\":\n",
    "            offense_words_matched = [y for y in offence_set if list(y)[0] == list(x)[0] ]\n",
    "            distance_track = [edit_distance(x,y) for y in offense_words_matched]\n",
    "            count += len([n for n in distance_track if n <= distance])\n",
    "    return count\n",
    "        \n",
    "def contains_offensive_words(text,count,stem,distance):\n",
    "    text_set = set(word_tokenize(text))\n",
    "    text_set_stemmed = set([stemmer.stem(x) for x in word_tokenize(text)])\n",
    "    if distance == 0:\n",
    "        if stem:\n",
    "            matched_words = text_set_stemmed.intersection(offense_words_stemmed)\n",
    "        else:\n",
    "            matched_words = text_set.intersection(offense_words)\n",
    "        if len(matched_words) >= count:\n",
    "            return 'HOF'\n",
    "        else:\n",
    "            return 'NOT'\n",
    "    else:\n",
    "        if stem:\n",
    "            matched_words = text_set_stemmed.intersection(offense_words_stemmed)\n",
    "            if len(matched_words) >= count:\n",
    "                return 'HOF'\n",
    "            else:\n",
    "                match_count = distance_match(text_set_stemmed,offense_words_stemmed,distance)\n",
    "        else:\n",
    "            match_count = distance_match(text_set,offense_words,distance)\n",
    "            \n",
    "        if match_count >= count:\n",
    "            return 'HOF'\n",
    "        else:\n",
    "            return 'NOT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting only with rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        HOF       0.82      0.93      0.87       338\n",
      "        NOT       0.92      0.82      0.87       370\n",
      "\n",
      "avg / total       0.88      0.87      0.87       708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_rules_pred = df_test.apply(lambda row: contains_offensive_words(row.text,1,False,0), axis=1)\n",
    "df_test['rules-task1'] = y_rules_pred.tolist()\n",
    "print(classification_report(df_test['task1'], y_rules_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing where svm goes wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-7d5f4261a10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_t1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'svm-task1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#& (df_test['rules-task1']=='NOT')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "df_test[['text',]].loc[(df_test['label_t1']==0) & (df_test['svm-task1']==1) ].tolist()\n",
    "#& (df_test['rules-task1']=='NOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seems like external rules are not effecting results of svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training svn for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding label column\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(df_train['task2'].values)\n",
    "\n",
    "df_train['label_t2'] = le.transform(df_train['task2'].values)\n",
    "df_test['label_t2'] = le.transform(df_test['task2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['label_t2'].values\n",
    "\n",
    "cv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True, stop_words='english')\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "SVM_Task2 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\n",
    "SVM_Task2.fit(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SVM_Task2 = SVM_Task2.predict_proba(cv.transform(df_test['text'].values))\n",
    "df_test['svm-task2'] =(predictions_SVM_Task2.tolist())\n",
    "print(classification_report(df_test['label_t2'],predictions_SVM_Task2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low score for 0 and 2 class because of low training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining TASK 2 with TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Task 2 results of NONE are almost equivalent to that of on TASK 1,\n",
    "# so combining both may not provide much improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Task 1 and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting naive bayes results\n",
    "model_lr_results =LR_Task1.predict_proba(cv.transform(df_test['text'].values))\n",
    "# getting rule based results\n",
    "rule_HOF_results =df_test.apply(lambda row: contains_offensive_words(row.text,3,True,0), axis=1)\n",
    "rule_NOT_results =df_test.apply(lambda row: contains_offensive_words(row.text,1,True,0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        HOF       0.94      0.88      0.91       338\n",
      "        NOT       0.90      0.95      0.92       370\n",
      "\n",
      "avg / total       0.92      0.92      0.92       708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "threshold_NOT = 0.9\n",
    "threshold_HOF = 0.9\n",
    "final_lr_predictions = []\n",
    "\n",
    "def compare(x,rule_HOF,rule_NOT):\n",
    "    \n",
    "    if rule_HOF == 'HOF':\n",
    "        return 'HOF'\n",
    "    elif rule_NOT == 'NOT':\n",
    "        return 'NOT'\n",
    "#     elif x[0]>threshold_NOT:\n",
    "#         return 'NOT'\n",
    "#     elif x[1]>threshold_HOF:\n",
    "#         return 'HOF'\n",
    "    else:\n",
    "        if x[0]>x[1]:\n",
    "            return 'NOT'\n",
    "        else:\n",
    "            return 'HOF'\n",
    "#         return rule_HOF\n",
    "        \n",
    "        \n",
    "for x in model_lr_results:\n",
    "    final_lr_predictions.append(compare(x,rule_HOF_results[index],rule_NOT_results[index]))\n",
    "    index += 1\n",
    "    \n",
    "print(classification_report(df_test['task1'], final_lr_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full offense people make video games almost never understand whats enjoyable video games',\n",
       " 'geterdone definitely judge mouth piece',\n",
       " 'sick republicans endorsing positions simply appease media left media loves republicans th',\n",
       " 'americans like excuse killing thousands children middle east draw line getting rid cl',\n",
       " 'lilbittyya mfers sick asl thats',\n",
       " 'brennan always looks like hes smelling bad fish hes bitter angry probably bec',\n",
       " 'bossie devastating false narrative left wing media feed american people two years',\n",
       " 'shadesofday ppl think nuts say want date marry like dont gotta get married fuckin tomorrow ev',\n",
       " 'thobykov sick stairs',\n",
       " 'good defence attorney would tell piss prosecutor',\n",
       " 'rudy youre either lunatic senile',\n",
       " 'look things like come not blame stupid handler account see ones retweet lo',\n",
       " 'going cody nerd haha whats douche bag noel highschool remember guys',\n",
       " 'drogon worst szn ever nigga brothers died mans mental health shambles',\n",
       " 'vid pretty liar',\n",
       " 'turtle youre dumbass',\n",
       " 'bull shite youre going assert facts least actually interested reflecting reality youre asserti',\n",
       " 'middle ground fuckin failure goddam old fool',\n",
       " 'lakers fans outside staples making look stupid front office isnt even bad shape espn ju',\n",
       " 'get females trouble find',\n",
       " 'done firefighters dues sucking union ever get paid zero',\n",
       " 'piece',\n",
       " 'know make america great tell hell',\n",
       " 'idiotic mpsthey hell',\n",
       " 'conservatives abortion want ban conservatives gay marriage want ban conservatives immigration w',\n",
       " 'stressing everything moment know youll well hoping know think',\n",
       " 'ciara never emo',\n",
       " 'senate president bukola saraki deputy senate president ike ekweremadu hailed nigerian workers playi',\n",
       " 'turning point usas unlv president gonna run country white power ns',\n",
       " 'alina see yapping cuz actually talented oopsie yo',\n",
       " 'boners days two get together',\n",
       " 'look bernies face priceless spent years developing con dumb',\n",
       " 'not single arab ever shot school isnt racist stupid every school shooter looked like alla',\n",
       " 'war criminal make sick',\n",
       " 'bana jihadis sponsored pakistan spread indiathey need hounded thrown get',\n",
       " 'chance opportunity ever rest spineless gopi think might fina',\n",
       " 'totally agree like always went way help make big goes rotten shit',\n",
       " 'abortion ban birth control ban sex ban guns look banning things never works people always f',\n",
       " 'yes unfortunately stupid',\n",
       " 'stop saying love simply want get legs honest tell want might give',\n",
       " 'twitter got yall stupid better scorer',\n",
       " 'one fearful words dont know stupid may sound might not',\n",
       " 'people thread acting like alienating audience big deal need look dicks sp',\n",
       " 'sick mother felt way would heso pathetic excuses ju',\n",
       " 'augur weak minded makes remoaners insufferable twats not getting way']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'].loc[(df_test['label_t1']==1) & (df_test['lr-task1']==0) ].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>ID</th>\n",
       "      <th>label_t1</th>\n",
       "      <th>svm-task1</th>\n",
       "      <th>lr-task1</th>\n",
       "      <th>rules-task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1126918111926657024</td>\n",
       "      <td>watching trailer commented see theyre still us...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123797864319279109</td>\n",
       "      <td>still comparing taylor swift beyonc ever gonna...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_1567</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123475024520912899</td>\n",
       "      <td>yall stick damn crying tweets ass honestly fee...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_en_335</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1127008771795030017</td>\n",
       "      <td>seems several gotham residents decided holiday...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1123580729366011904</td>\n",
       "      <td>everything youre going right temporary hang th...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1126767414740340736</td>\n",
       "      <td>thanks pic sir mark nabuhay ulit ang yellow he...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1123504804075048960</td>\n",
       "      <td>shit dying</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_5266</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1123799890159722497</td>\n",
       "      <td>isabella tell</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2573</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1130090532452130818</td>\n",
       "      <td>theres not one time drink doesnt get spilled f...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1123826024867930112</td>\n",
       "      <td>youre going haters youre going lovers onebiglo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1130342006138363907</td>\n",
       "      <td>niga probably bitch</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_en_1831</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1130332074030841856</td>\n",
       "      <td>agree exactly parallel would okay pair actors ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_5251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1126889380923281410</td>\n",
       "      <td>niggas talk fuck anything size shape ugly not ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_en_3985</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1130010572232187904</td>\n",
       "      <td>bitch miss u</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1130301145224798217</td>\n",
       "      <td>brienne petty wrote died protecting sister sex...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_753</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1130076351518642176</td>\n",
       "      <td>thats boys fake relationship fuck</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1130252394837749761</td>\n",
       "      <td>losing best friend probably one worst pains pe...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1130165862177083392</td>\n",
       "      <td>idk youre cappin acting like dont unprotected ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2487</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1123690360084017154</td>\n",
       "      <td>monsoon delayed modi piece shit masoodazhar de...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_en_2175</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1126835362490408960</td>\n",
       "      <td>field trip buddy great hung friend mom name ni...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1126736175568281600</td>\n",
       "      <td>abortion rights arent whether get one live nar...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_4949</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1123551830619901952</td>\n",
       "      <td>bet youre not youre supposed right</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2749</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1130032806258593792</td>\n",
       "      <td>think responsibility tell everyone meditation ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1123775408019734530</td>\n",
       "      <td>not damn day passes not think</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_4549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1130327930083729408</td>\n",
       "      <td>beauty grace shell punch face bnha detailed</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3027</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1123560688989945856</td>\n",
       "      <td>holy fuck</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_4464</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1126924768286916608</td>\n",
       "      <td>coates impeachment barr spokesmen never conduc...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_1037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1127019454662156288</td>\n",
       "      <td>temi date man doesnt eat pussy saddest shit ever</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_4501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1126952345798094848</td>\n",
       "      <td>full offense people make video games almost ne...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_en_5002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1126867167910219777</td>\n",
       "      <td>fuck please let suck cock dry clean</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_1780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>1127077029873164288</td>\n",
       "      <td>youre mood facial dont patience set hookup wat...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1130297588467601408</td>\n",
       "      <td>crazy years fucking person act like yall never...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>1123603571570659328</td>\n",
       "      <td>cris look really sick dani skamespaa</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1126997505894440960</td>\n",
       "      <td>fucking love</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_5081</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>1130054109116207105</td>\n",
       "      <td>stop talking start stop talking start working ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_5183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>1126812612568571904</td>\n",
       "      <td>holly shit</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_982</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>1130075164513673216</td>\n",
       "      <td>oce fucking come every esports dont fuckin sle...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_4674</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1123691240887922688</td>\n",
       "      <td>youre welcome sir</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>1130297458431614976</td>\n",
       "      <td>daenerys snatches throne satan becomes queen h...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1130065135949762562</td>\n",
       "      <td>glad champ knew would back</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>1126820082640982018</td>\n",
       "      <td>kraybaybay like actually need dad</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_1133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1127030137592209408</td>\n",
       "      <td>block party bileys mii head ass pit</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_3835</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>1126814541965398016</td>\n",
       "      <td>want let else fade away</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>1123779023484669954</td>\n",
       "      <td>piece power either chamber congress censure pr...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_5078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>1126915578541891588</td>\n",
       "      <td>blow angis tweet fake ass</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>1130190788904669185</td>\n",
       "      <td>problem allowing people religion choice not re...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>1123480644888276992</td>\n",
       "      <td>cannae believe shambles ballot final fickwit d...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_1980</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1123600316774019073</td>\n",
       "      <td>god get sugar daddy starbucks line mgm</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2934</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>1126909400327782400</td>\n",
       "      <td>augur weak minded makes remoaners insufferable...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_en_4760</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>1123745771042504704</td>\n",
       "      <td>dick dawg delusional unbelievable many stupid ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_4832</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1123776670480179200</td>\n",
       "      <td>holy shoot theyre nextehbsjsk</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>1123552870807351298</td>\n",
       "      <td>yall ever read message someone amp think see d...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>hasoc_2020_en_2688</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1130313010919034880</td>\n",
       "      <td>people sick</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>1126848473880498178</td>\n",
       "      <td>never catch lake lanier research years ago</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>1130188674967191554</td>\n",
       "      <td>shit needed hear like think beau</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>hasoc_2020_en_4709</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1123764637021945856</td>\n",
       "      <td>people followed automatically checked</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1126970465199771648</td>\n",
       "      <td>know working ill need</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>1127078015555362818</td>\n",
       "      <td>baby cousin help kaya kick leukemias ass</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>hasoc_2020_en_4495</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1130248645138366464</td>\n",
       "      <td>want haunted lighthouse tour sometime</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_3935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1127031655926050816</td>\n",
       "      <td>still cant get picture siszamo theyre honestly...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>hasoc_2020_en_2873</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>708 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1126918111926657024  watching trailer commented see theyre still us...   \n",
       "1    1123797864319279109  still comparing taylor swift beyonc ever gonna...   \n",
       "2    1123475024520912899  yall stick damn crying tweets ass honestly fee...   \n",
       "3    1127008771795030017  seems several gotham residents decided holiday...   \n",
       "4    1123580729366011904  everything youre going right temporary hang th...   \n",
       "5    1126767414740340736  thanks pic sir mark nabuhay ulit ang yellow he...   \n",
       "6    1123504804075048960                                         shit dying   \n",
       "7    1123799890159722497                                      isabella tell   \n",
       "8    1130090532452130818  theres not one time drink doesnt get spilled f...   \n",
       "9    1123826024867930112  youre going haters youre going lovers onebiglo...   \n",
       "10   1130342006138363907                                niga probably bitch   \n",
       "11   1130332074030841856  agree exactly parallel would okay pair actors ...   \n",
       "12   1126889380923281410  niggas talk fuck anything size shape ugly not ...   \n",
       "13   1130010572232187904                                       bitch miss u   \n",
       "14   1130301145224798217  brienne petty wrote died protecting sister sex...   \n",
       "15   1130076351518642176                  thats boys fake relationship fuck   \n",
       "16   1130252394837749761  losing best friend probably one worst pains pe...   \n",
       "17   1130165862177083392  idk youre cappin acting like dont unprotected ...   \n",
       "18   1123690360084017154  monsoon delayed modi piece shit masoodazhar de...   \n",
       "19   1126835362490408960  field trip buddy great hung friend mom name ni...   \n",
       "20   1126736175568281600  abortion rights arent whether get one live nar...   \n",
       "21   1123551830619901952                 bet youre not youre supposed right   \n",
       "22   1130032806258593792  think responsibility tell everyone meditation ...   \n",
       "23   1123775408019734530                      not damn day passes not think   \n",
       "24   1130327930083729408        beauty grace shell punch face bnha detailed   \n",
       "25   1123560688989945856                                          holy fuck   \n",
       "26   1126924768286916608  coates impeachment barr spokesmen never conduc...   \n",
       "27   1127019454662156288   temi date man doesnt eat pussy saddest shit ever   \n",
       "28   1126952345798094848  full offense people make video games almost ne...   \n",
       "29   1126867167910219777                fuck please let suck cock dry clean   \n",
       "..                   ...                                                ...   \n",
       "678  1127077029873164288  youre mood facial dont patience set hookup wat...   \n",
       "679  1130297588467601408  crazy years fucking person act like yall never...   \n",
       "680  1123603571570659328               cris look really sick dani skamespaa   \n",
       "681  1126997505894440960                                       fucking love   \n",
       "682  1130054109116207105  stop talking start stop talking start working ...   \n",
       "683  1126812612568571904                                         holly shit   \n",
       "684  1130075164513673216  oce fucking come every esports dont fuckin sle...   \n",
       "685  1123691240887922688                                  youre welcome sir   \n",
       "686  1130297458431614976  daenerys snatches throne satan becomes queen h...   \n",
       "687  1130065135949762562                         glad champ knew would back   \n",
       "688  1126820082640982018                  kraybaybay like actually need dad   \n",
       "689  1127030137592209408                block party bileys mii head ass pit   \n",
       "690  1126814541965398016                            want let else fade away   \n",
       "691  1123779023484669954  piece power either chamber congress censure pr...   \n",
       "692  1126915578541891588                          blow angis tweet fake ass   \n",
       "693  1130190788904669185  problem allowing people religion choice not re...   \n",
       "694  1123480644888276992  cannae believe shambles ballot final fickwit d...   \n",
       "695  1123600316774019073             god get sugar daddy starbucks line mgm   \n",
       "696  1126909400327782400  augur weak minded makes remoaners insufferable...   \n",
       "697  1123745771042504704  dick dawg delusional unbelievable many stupid ...   \n",
       "698  1123776670480179200                      holy shoot theyre nextehbsjsk   \n",
       "699  1123552870807351298  yall ever read message someone amp think see d...   \n",
       "700  1130313010919034880                                        people sick   \n",
       "701  1126848473880498178         never catch lake lanier research years ago   \n",
       "702  1130188674967191554                   shit needed hear like think beau   \n",
       "703  1123764637021945856              people followed automatically checked   \n",
       "704  1126970465199771648                              know working ill need   \n",
       "705  1127078015555362818           baby cousin help kaya kick leukemias ass   \n",
       "706  1130248645138366464              want haunted lighthouse tour sometime   \n",
       "707  1127031655926050816  still cant get picture siszamo theyre honestly...   \n",
       "\n",
       "    task1 task2                  ID  label_t1  svm-task1  lr-task1 rules-task1  \n",
       "0     NOT  NONE  hasoc_2020_en_4930         0          0         0         NOT  \n",
       "1     NOT  NONE  hasoc_2020_en_1567         0          0         0         NOT  \n",
       "2     HOF  HATE   hasoc_2020_en_335         1          1         1         HOF  \n",
       "3     NOT  NONE  hasoc_2020_en_4127         0          0         0         NOT  \n",
       "4     NOT  NONE  hasoc_2020_en_3552         0          0         0         NOT  \n",
       "5     NOT  NONE  hasoc_2020_en_2167         0          0         0         NOT  \n",
       "6     HOF  PRFN  hasoc_2020_en_5266         1          1         1         HOF  \n",
       "7     NOT  NONE  hasoc_2020_en_2573         0          0         0         NOT  \n",
       "8     NOT  NONE   hasoc_2020_en_231         0          0         0         NOT  \n",
       "9     NOT  NONE  hasoc_2020_en_4303         0          0         0         NOT  \n",
       "10    HOF  OFFN  hasoc_2020_en_1831         1          1         1         HOF  \n",
       "11    NOT  NONE  hasoc_2020_en_5251         0          0         0         HOF  \n",
       "12    HOF  HATE  hasoc_2020_en_3985         1          1         1         HOF  \n",
       "13    HOF  PRFN    hasoc_2020_en_28         1          1         1         HOF  \n",
       "14    HOF  PRFN   hasoc_2020_en_753         1          1         1         HOF  \n",
       "15    HOF  PRFN   hasoc_2020_en_808         1          1         1         HOF  \n",
       "16    NOT  NONE  hasoc_2020_en_3270         0          0         0         NOT  \n",
       "17    NOT  NONE  hasoc_2020_en_2487         0          1         0         HOF  \n",
       "18    HOF  OFFN  hasoc_2020_en_2175         1          1         1         HOF  \n",
       "19    NOT  NONE  hasoc_2020_en_2648         0          0         0         NOT  \n",
       "20    HOF  PRFN  hasoc_2020_en_4949         1          1         1         HOF  \n",
       "21    NOT  NONE  hasoc_2020_en_2749         0          0         0         NOT  \n",
       "22    NOT  NONE  hasoc_2020_en_4835         0          0         0         NOT  \n",
       "23    HOF  PRFN  hasoc_2020_en_4549         1          1         1         HOF  \n",
       "24    NOT  NONE  hasoc_2020_en_3027         0          0         0         NOT  \n",
       "25    HOF  PRFN  hasoc_2020_en_4464         1          1         1         HOF  \n",
       "26    NOT  NONE  hasoc_2020_en_1037         0          0         0         NOT  \n",
       "27    HOF  PRFN  hasoc_2020_en_4501         1          1         1         HOF  \n",
       "28    HOF  HATE  hasoc_2020_en_5002         1          0         0         NOT  \n",
       "29    HOF  PRFN  hasoc_2020_en_1780         1          1         1         HOF  \n",
       "..    ...   ...                 ...       ...        ...       ...         ...  \n",
       "678   NOT  NONE  hasoc_2020_en_4762         0          0         0         NOT  \n",
       "679   HOF  PRFN    hasoc_2020_en_43         1          1         1         HOF  \n",
       "680   NOT  NONE  hasoc_2020_en_3418         0          0         0         NOT  \n",
       "681   HOF  PRFN  hasoc_2020_en_5081         1          1         1         HOF  \n",
       "682   NOT  NONE  hasoc_2020_en_5183         0          0         0         HOF  \n",
       "683   HOF  PRFN   hasoc_2020_en_982         1          1         1         HOF  \n",
       "684   HOF  PRFN  hasoc_2020_en_4674         1          1         1         HOF  \n",
       "685   NOT  NONE  hasoc_2020_en_3864         0          0         0         NOT  \n",
       "686   NOT  NONE  hasoc_2020_en_2710         0          0         0         HOF  \n",
       "687   NOT  NONE  hasoc_2020_en_2582         0          0         0         NOT  \n",
       "688   NOT  NONE  hasoc_2020_en_1133         0          0         0         NOT  \n",
       "689   HOF  PRFN  hasoc_2020_en_3835         1          1         1         HOF  \n",
       "690   NOT  NONE  hasoc_2020_en_4634         0          0         0         NOT  \n",
       "691   NOT  NONE  hasoc_2020_en_5078         0          0         0         NOT  \n",
       "692   HOF  PRFN    hasoc_2020_en_49         1          1         1         HOF  \n",
       "693   NOT  NONE  hasoc_2020_en_2238         0          0         0         NOT  \n",
       "694   HOF  PRFN  hasoc_2020_en_1980         1          1         1         HOF  \n",
       "695   NOT  NONE  hasoc_2020_en_2934         0          0         0         HOF  \n",
       "696   HOF  OFFN  hasoc_2020_en_4760         1          0         0         NOT  \n",
       "697   NOT  NONE  hasoc_2020_en_4832         0          1         1         HOF  \n",
       "698   NOT  NONE  hasoc_2020_en_2177         0          0         0         HOF  \n",
       "699   HOF  OFFN  hasoc_2020_en_2688         1          1         1         HOF  \n",
       "700   NOT  NONE  hasoc_2020_en_3332         0          0         0         NOT  \n",
       "701   NOT  NONE   hasoc_2020_en_466         0          0         0         NOT  \n",
       "702   HOF  PRFN  hasoc_2020_en_4709         1          1         1         HOF  \n",
       "703   NOT  NONE  hasoc_2020_en_3915         0          0         0         NOT  \n",
       "704   NOT  NONE  hasoc_2020_en_2139         0          0         0         NOT  \n",
       "705   HOF  HATE  hasoc_2020_en_4495         1          1         1         HOF  \n",
       "706   NOT  NONE  hasoc_2020_en_3935         0          0         0         NOT  \n",
       "707   NOT  NONE  hasoc_2020_en_2873         0          0         0         NOT  \n",
       "\n",
       "[708 rows x 9 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
